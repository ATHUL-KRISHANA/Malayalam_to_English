{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Malayalam to English Transilation\n",
    "Machine transilation has become an essential tool for bridging language barriers in our increasingly interconnected world.The main challenge of transilation between Malayalam and English  is distinct structure and cultural contexts.We use Hugging face to tackle this challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:11.462261Z",
     "iopub.status.busy": "2024-12-23T07:21:11.462062Z",
     "iopub.status.idle": "2024-12-23T07:21:11.465907Z",
     "shell.execute_reply": "2024-12-23T07:21:11.465148Z",
     "shell.execute_reply.started": "2024-12-23T07:21:11.462244Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import os\n",
    "import re\n",
    "# from sacrebleu import corpus_bleu\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "we use the dataset Hemanth-thunder/english-to-malayalam-mt from hugging face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:11.479937Z",
     "iopub.status.busy": "2024-12-23T07:21:11.479702Z",
     "iopub.status.idle": "2024-12-23T07:21:12.097992Z",
     "shell.execute_reply": "2024-12-23T07:21:12.097225Z",
     "shell.execute_reply.started": "2024-12-23T07:21:11.479918Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ml'],\n",
       "        num_rows: 5924426\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"Hemanth-thunder/english-to-malayalam-mt\")\n",
    "raw_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:12.099505Z",
     "iopub.status.busy": "2024-12-23T07:21:12.099178Z",
     "iopub.status.idle": "2024-12-23T07:21:12.107282Z",
     "shell.execute_reply": "2024-12-23T07:21:12.106475Z",
     "shell.execute_reply.started": "2024-12-23T07:21:12.099469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ml'],\n",
       "        num_rows: 200000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "raw_dataset = DatasetDict({\n",
    "    'train': raw_dataset['train'].select(range(200000))\n",
    "})\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:12.108933Z",
     "iopub.status.busy": "2024-12-23T07:21:12.108704Z",
     "iopub.status.idle": "2024-12-23T07:21:12.123974Z",
     "shell.execute_reply": "2024-12-23T07:21:12.123176Z",
     "shell.execute_reply.started": "2024-12-23T07:21:12.108904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': 'The plot of the movie revolves around the life of two cancer patients Kizie and Manny.',\n",
       " 'ml': '‡¥ï‡µç‡¥Ø‡¥æ‡¥®‡µç\\u200d‡¥∏‡¥±‡¥ø‡¥®‡µã‡¥ü‡µç ‡¥™‡µã‡¥∞‡¥æ‡¥ü‡µÅ‡¥®‡µç‡¥® ‡¥ï‡¥ø‡¥∏‡¥ø, ‡¥Æ‡¥æ‡¥®‡¥ø ‡¥é‡¥®‡µç‡¥®‡¥ø‡¥µ‡¥∞‡µÅ‡¥ü‡µÜ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Æ‡¥æ‡¥£‡µç ‡¥ö‡¥ø‡¥§‡µç‡¥∞‡¥Ç ‡¥™‡¥±‡¥Ø‡µÅ‡¥®‡µç‡¥®‡¥§‡µç.'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:12.125593Z",
     "iopub.status.busy": "2024-12-23T07:21:12.125145Z",
     "iopub.status.idle": "2024-12-23T07:21:12.143698Z",
     "shell.execute_reply": "2024-12-23T07:21:12.143118Z",
     "shell.execute_reply.started": "2024-12-23T07:21:12.125573Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_malayalam_text(example):\n",
    "    pattern = r\"[\\u2000-\\u200F]+\" #this will remove the zwj\n",
    "    text = re.sub(pattern, '', example['ml'])\n",
    "    text=text.lower()\n",
    "    text=re.sub(\"'\",'',text)\n",
    "    text = re.sub('[^\\u0D00-\\u0D7F]+', ' ', text) #all other character than malayalam\n",
    "    \n",
    "    example['ml']=text\n",
    "    return example\n",
    "\n",
    "raw_dataset['train'] = raw_dataset['train'].map(clean_malayalam_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:12.144918Z",
     "iopub.status.busy": "2024-12-23T07:21:12.144636Z",
     "iopub.status.idle": "2024-12-23T07:21:12.496071Z",
     "shell.execute_reply": "2024-12-23T07:21:12.495228Z",
     "shell.execute_reply.started": "2024-12-23T07:21:12.144891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡¥ï‡µç‡¥Ø‡¥æ‡¥®‡µç‡¥∏‡¥±‡¥ø‡¥®‡µã‡¥ü‡µç ‡¥™‡µã‡¥∞‡¥æ‡¥ü‡µÅ‡¥®‡µç‡¥® ‡¥ï‡¥ø‡¥∏‡¥ø ‡¥Æ‡¥æ‡¥®‡¥ø ‡¥é‡¥®‡µç‡¥®‡¥ø‡¥µ‡¥∞‡µÅ‡¥ü‡µÜ ‡¥ú‡µÄ‡¥µ‡¥ø‡¥§‡¥Æ‡¥æ‡¥£‡µç ‡¥ö‡¥ø‡¥§‡µç‡¥∞‡¥Ç ‡¥™‡¥±‡¥Ø‡µÅ‡¥®‡µç‡¥®‡¥§‡µç '"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train']['ml'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:12.497213Z",
     "iopub.status.busy": "2024-12-23T07:21:12.496975Z",
     "iopub.status.idle": "2024-12-23T07:21:12.723371Z",
     "shell.execute_reply": "2024-12-23T07:21:12.722648Z",
     "shell.execute_reply.started": "2024-12-23T07:21:12.497193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The plot of the movie revolves around the life of two cancer patients Kizie and Manny.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train']['en'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:12.724594Z",
     "iopub.status.busy": "2024-12-23T07:21:12.724194Z",
     "iopub.status.idle": "2024-12-23T07:21:21.692183Z",
     "shell.execute_reply": "2024-12-23T07:21:21.691365Z",
     "shell.execute_reply.started": "2024-12-23T07:21:12.724565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8162cf646b214f5fb1e0938709052acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_english(example):\n",
    "    text = re.sub(\"'\", '', example['en'])\n",
    "    text=text.lower()\n",
    "    text=re.sub('[^a-z]+',' ',text)\n",
    "    example['en']=text\n",
    "    return example\n",
    "raw_dataset['train'] = raw_dataset['train'].map(clean_english)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:21.694219Z",
     "iopub.status.busy": "2024-12-23T07:21:21.693984Z",
     "iopub.status.idle": "2024-12-23T07:21:21.891205Z",
     "shell.execute_reply": "2024-12-23T07:21:21.890537Z",
     "shell.execute_reply.started": "2024-12-23T07:21:21.694197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the plot of the movie revolves around the life of two cancer patients kizie and manny '"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train']['en'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the pretrained model *Helsinki-NLP/opus-mt-ml-en*,this model will help to tokenize and transilate to sequence to sequence task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:21:54.198241Z",
     "iopub.status.busy": "2024-12-23T07:21:54.197941Z",
     "iopub.status.idle": "2024-12-23T07:21:54.202300Z",
     "shell.execute_reply": "2024-12-23T07:21:54.201563Z",
     "shell.execute_reply.started": "2024-12-23T07:21:54.198217Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"Helsinki-NLP/opus-mt-ml-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:22:11.083240Z",
     "iopub.status.busy": "2024-12-23T07:22:11.082969Z",
     "iopub.status.idle": "2024-12-23T07:22:12.361048Z",
     "shell.execute_reply": "2024-12-23T07:22:12.360087Z",
     "shell.execute_reply.started": "2024-12-23T07:22:11.083218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf97a12212346638652b90c2acd4614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38acdb63df4498fa6796408c595cc34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0740505506048b7a1bdec25f111def8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48148acf5d7414b994c1ca9c5b91f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/818k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5339015655764d4181369aceff35025b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:23:48.840034Z",
     "iopub.status.busy": "2024-12-23T07:23:48.839661Z",
     "iopub.status.idle": "2024-12-23T07:23:48.845197Z",
     "shell.execute_reply": "2024-12-23T07:23:48.844368Z",
     "shell.execute_reply.started": "2024-12-23T07:23:48.840006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ml'],\n",
       "    num_rows: 200000\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset=raw_dataset['train']\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:28:50.174968Z",
     "iopub.status.busy": "2024-12-23T07:28:50.174644Z",
     "iopub.status.idle": "2024-12-23T07:28:50.179562Z",
     "shell.execute_reply": "2024-12-23T07:28:50.178699Z",
     "shell.execute_reply.started": "2024-12-23T07:28:50.174941Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length=128\n",
    "def preprocess(example):\n",
    "    text=[ml for ml in example['ml']]\n",
    "    labels=[en for en in example['en']]\n",
    "    model_input=tokenizer(text,max_length=max_length)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        label=tokenizer(labels,max_length=max_length)\n",
    "    model_input['labels']=label['input_ids']\n",
    "    return model_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:28:52.257565Z",
     "iopub.status.busy": "2024-12-23T07:28:52.257271Z",
     "iopub.status.idle": "2024-12-23T07:29:21.146087Z",
     "shell.execute_reply": "2024-12-23T07:29:21.145257Z",
     "shell.execute_reply.started": "2024-12-23T07:28:52.257542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a736246d5e7049c3888ea64dbb6642e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['en', 'ml', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 200000\n",
       "})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset=raw_dataset.map(preprocess,batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:29:56.428342Z",
     "iopub.status.busy": "2024-12-23T07:29:56.428047Z",
     "iopub.status.idle": "2024-12-23T07:29:56.485005Z",
     "shell.execute_reply": "2024-12-23T07:29:56.484323Z",
     "shell.execute_reply.started": "2024-12-23T07:29:56.428316Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'ml', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 160000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'ml', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=tokenized_dataset.train_test_split(test_size=0.2)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:30:47.498453Z",
     "iopub.status.busy": "2024-12-23T07:30:47.498164Z",
     "iopub.status.idle": "2024-12-23T07:30:47.502013Z",
     "shell.execute_reply": "2024-12-23T07:30:47.501344Z",
     "shell.execute_reply.started": "2024-12-23T07:30:47.498430Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset=dataset['train']\n",
    "test_dataset=dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:32:17.648425Z",
     "iopub.status.busy": "2024-12-23T07:32:17.648145Z",
     "iopub.status.idle": "2024-12-23T07:32:21.002168Z",
     "shell.execute_reply": "2024-12-23T07:32:21.001362Z",
     "shell.execute_reply.started": "2024-12-23T07:32:17.648403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfab7db75c9a4877a0318842f67d257b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d35c801998b4095840ecb7576e2fc3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:32:51.853421Z",
     "iopub.status.busy": "2024-12-23T07:32:51.853113Z",
     "iopub.status.idle": "2024-12-23T07:32:52.025354Z",
     "shell.execute_reply": "2024-12-23T07:32:52.024493Z",
     "shell.execute_reply.started": "2024-12-23T07:32:51.853398Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./malayalam_to_english_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    predict_with_generate=True,\n",
    "    save_total_limit=2,  # Limit saved checkpoints\n",
    "    generation_max_length=128\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:33:37.433223Z",
     "iopub.status.busy": "2024-12-23T07:33:37.432915Z",
     "iopub.status.idle": "2024-12-23T07:33:37.436835Z",
     "shell.execute_reply": "2024-12-23T07:33:37.435891Z",
     "shell.execute_reply.started": "2024-12-23T07:33:37.433199Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors = \"pt\", pad_to_multiple_of=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:34:02.817982Z",
     "iopub.status.busy": "2024-12-23T07:34:02.817687Z",
     "iopub.status.idle": "2024-12-23T07:34:03.416749Z",
     "shell.execute_reply": "2024-12-23T07:34:03.416085Z",
     "shell.execute_reply.started": "2024-12-23T07:34:02.817959Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T07:34:10.268557Z",
     "iopub.status.busy": "2024-12-23T07:34:10.268150Z",
     "iopub.status.idle": "2024-12-23T09:41:28.189415Z",
     "shell.execute_reply": "2024-12-23T09:41:28.188521Z",
     "shell.execute_reply.started": "2024-12-23T07:34:10.268520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 2:07:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.378400</td>\n",
       "      <td>3.201760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.139900</td>\n",
       "      <td>3.014739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.985600</td>\n",
       "      <td>2.967042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[63223]], 'forced_eos_token_id': 0}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[63223]], 'forced_eos_token_id': 0}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[63223]], 'forced_eos_token_id': 0}\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[63223]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=3.2898853190104167, metrics={'train_runtime': 7636.6443, 'train_samples_per_second': 62.855, 'train_steps_per_second': 1.964, 'total_flos': 1.627121516544e+16, 'train_loss': 3.2898853190104167, 'epoch': 3.0})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing Model\n",
    "To perform inference with your model, you'll need to convert your input text into numerical vectors using the tokenizer . Once your input is prepared, you can use the generate function to obtain predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T10:31:40.670432Z",
     "iopub.status.busy": "2024-12-23T10:31:40.670090Z",
     "iopub.status.idle": "2024-12-23T10:31:40.754412Z",
     "shell.execute_reply": "2024-12-23T10:31:40.753374Z",
     "shell.execute_reply.started": "2024-12-23T10:31:40.670406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  ‡¥∏‡µÅ‡¥ñ‡¥Æ‡¥æ‡¥£‡µá‡¥æ\n",
      "Translated Output: how are you\n"
     ]
    }
   ],
   "source": [
    "def translate_new_input(model,tokenizer,input_text,max_length=128):\n",
    "    device=model.device\n",
    "    model.to(device)  \n",
    "    inputs=tokenizer(input_text,return_tensors=\"pt\",truncation=True, max_length=max_length).to(device)\n",
    "    outputs=model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "    translated_text=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "new_input =\" ‡¥∏‡µÅ‡¥ñ‡¥Æ‡¥æ‡¥£‡µá‡¥æ\"  # Malayalam input\n",
    "translated_output = translate_new_input(model, tokenizer, new_input)\n",
    "print(f\"Input: {new_input}\")\n",
    "print(f\"Translated Output: {translated_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T10:03:07.394923Z",
     "iopub.status.busy": "2024-12-23T10:03:07.394498Z",
     "iopub.status.idle": "2024-12-23T10:03:08.356081Z",
     "shell.execute_reply": "2024-12-23T10:03:08.354924Z",
     "shell.execute_reply.started": "2024-12-23T10:03:07.394889Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[63223]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/my_model/tokenizer_config.json',\n",
       " '/kaggle/working/my_model/special_tokens_map.json',\n",
       " '/kaggle/working/my_model/vocab.json',\n",
       " '/kaggle/working/my_model/source.spm',\n",
       " '/kaggle/working/my_model/target.spm',\n",
       " '/kaggle/working/my_model/added_tokens.json')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training\n",
    "\n",
    "# Save the model\n",
    "output_dir = \"/kaggle/working/my_model\"  # Directory in the Kaggle environment where you can save files\n",
    "\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "# Optionally, save the tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T10:05:32.050785Z",
     "iopub.status.busy": "2024-12-23T10:05:32.050402Z",
     "iopub.status.idle": "2024-12-23T10:05:33.331655Z",
     "shell.execute_reply": "2024-12-23T10:05:33.330763Z",
     "shell.execute_reply.started": "2024-12-23T10:05:32.050744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model1 = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/working/my_model\")\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"/kaggle/working/my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-23T10:07:00.834910Z",
     "iopub.status.busy": "2024-12-23T10:07:00.834573Z",
     "iopub.status.idle": "2024-12-23T10:07:01.797524Z",
     "shell.execute_reply": "2024-12-23T10:07:01.796620Z",
     "shell.execute_reply.started": "2024-12-23T10:07:00.834886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ‡¥±‡µã‡¥∏‡¥æ‡¥¶‡¥≥‡¥ô‡µç‡¥ô‡¥≥‡¥æ‡µΩ ‡¥™‡µä‡¥§‡¥ø‡¥û‡µç‡¥û ‡¥í‡¥∞‡µÅ ‡¥∂‡¥∞‡µÄ‡¥∞‡¥Ç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥µ‡¥æ‡¥Ø ‡¥™‡µÇ‡¥µ‡¥ø‡¥ü‡µç‡¥ü‡¥§‡µç ‡¥é‡¥µ‡¥ø‡¥ü‡µÜ‡¥Ø‡¥æ‡¥£‡µÜ‡¥®‡µç‡¥®‡µç ‡¥Ö‡¥ü‡¥Ø‡¥æ‡¥≥‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡µÅ‡¥®‡µç‡¥®‡µÅ\n",
      "Translated Output: a body covered with rock has been marked by your mouth\n"
     ]
    }
   ],
   "source": [
    "def translate_new_input(model,tokenizer,input_text,max_length=128):\n",
    "    device=model1.device\n",
    "    model1.to(device)  \n",
    "    inputs=tokenizer1(input_text,return_tensors=\"pt\",truncation=True, max_length=max_length).to(device)\n",
    "    outputs=model1.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "    translated_text=tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "new_input = \"‡¥±‡µã‡¥∏‡¥æ‡¥¶‡¥≥‡¥ô‡µç‡¥ô‡¥≥‡¥æ‡µΩ ‡¥™‡µä‡¥§‡¥ø‡¥û‡µç‡¥û ‡¥í‡¥∞‡µÅ ‡¥∂‡¥∞‡µÄ‡¥∞‡¥Ç ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ ‡¥µ‡¥æ‡¥Ø ‡¥™‡µÇ‡¥µ‡¥ø‡¥ü‡µç‡¥ü‡¥§‡µç ‡¥é‡¥µ‡¥ø‡¥ü‡µÜ‡¥Ø‡¥æ‡¥£‡µÜ‡¥®‡µç‡¥®‡µç ‡¥Ö‡¥ü‡¥Ø‡¥æ‡¥≥‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥§‡µç‡¥§‡µÅ‡¥®‡µç‡¥®‡µÅ\"  # Malayalam input\n",
    "translated_output = translate_new_input(model, tokenizer, new_input)\n",
    "print(f\"Input: {new_input}\")\n",
    "print(f\"Translated Output: {translated_output}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
